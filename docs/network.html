<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="utf-8">
    <title>Creating a Network</title>
    <link href="style.css" rel="stylesheet">
</head>

<body>
<div class="sidenavbar">
    <a href="index.html">Home Page</a>
    <a href="network.html">XOR Neural Network</a>
    <p>Layers</p>
    <a href="layer_dense.html">Dense Layer</a>
    <a href="layer_activation.html">Activation Layer</a>
    <p></p>
    <a href="activation_functions.html">Activation functions</a>
</div>

<div class="sidelist">
    <a href="#Requirements">Requirements</a>
    <a href="#StructureOfTheCode">Structure of the code</a>
</div>

<div class="main">
    <h1>> Creating a Neural Network</h1>
    <div class="part">
    <h2 id="Requirements">Requirements</h2>
    <p>
        You need to have V installed, you can verify that by running <i>v run .</i>.
        You also need to have the vsl module installed, you can install it with <i>v install vsl</i>.
        Then you need to clone the <a href="https://github.com/Eliyaan/NeuralNetworks-V-Module">module's repo</a> and create a v file in it that we are going to use.
    </p>
    </div>
    <div class="part">
    <h2 id="StructureOfTheCode">Structure of the code</h2>
    <p>
        We are going to create a neural network to train a neural network that will be able to perform the XOR logic gate. The result can be found <a href="https://github.com/Eliyaan/NeuralNetworks-V-Module/blob/main/examples/train_xor.v" target="_blank">here</a>.
    </p>
    <p>
        First we are going to import the neural network module and create a main function.
        Then we create the neural network that we are going to train.
        The 0 is the seed for the random weights and biases to be able to get the same neural network at each run.
    </p>
    <p class="snippet">
import neural_networks as nn

fn main() {
    mut model := nn.NeuralNetwork.new(0)
}
    </p>
    <p>
        Then we add the layers that we want our network to have.
        We need our network to have 2 inputs and 1 output to match the XOR gate.
        So we will first add a <a href="layer_dense.html">Dense layer</a> with 2 inputs and 3 outputs, 3 is arbitrary but works well.
        The two numbers after the number of inputs/outputs is the range for the initialisation of random weights and biases.
    </p>
    <p>
        Then an <a href="layer_activation.html">Activation layer</a>, the Dense and Activation layers are complementary so we will add one Activation per Dense layer.
        The activation function that we will use for this layer is leaky relu, as it is convenient.
        We add a second Dense layer with 3 input and 1 output and the Activation layer that goes with it.
    </p>
    <p class="snippet">
model.add_layer(nn.Dense.new(2, 3, 0.7, 0.65))
model.add_layer(nn.Activation.new(.leaky_relu))
model.add_layer(nn.Dense.new(3, 1, 0.6, 0.65))
model.add_layer(nn.Activation.new(.leaky_relu))
    </p>
    <p>
        Then we need to create the parametters for the training.
        The learning rate, momentum, number of epochs are found by trial and error and these work well.
        The cost function that we will use is the Mean Squared Error (MSE).
    </p>
    <p>
        We then add the dataset that the network will use for it's training.
        And same for the testing, in a real example the test data is unseen during the training to be able to see how well the networks does in an unseen situation
        but as we have only 4 different possible inputs we can not show unseen data to the network so we will use the same data.
    </p>
    <p>
        The neural newtork will print it's performance every <i>print_interval</i> epochs.
        For the test parameters, every <i>training_interval</i> epochs it will run the test dataset and print the results from the <i>print_start</i>th element of the test dataset to the <i>print_end</i>th one.
    </p>
    <p class="snippet">
training_parameters := nn.BackpropTrainingParams{
    learning_rate: 0.37
    momentum: 0.9
    nb_epochs: 300
    print_interval: 25
    cost_function: .mse // mean squared error
    training: nn.Dataset {
        inputs: [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]
        expected_outputs: [[0.0], [1.0], [1.0], [0.0]]
    }
    test: nn.Dataset {
        inputs: [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]
        expected_outputs: [[0.0], [1.0], [1.0], [0.0]]
    }
    test_params: nn.TestParams{
        print_start: 0
        print_end: 3
        training_interval: 100
    }
}
    </p>
    <p>
        Now it's the time to train the network!
    </p>
    <p class="snippet">
model.train(training_parameters)
    </p>
    <p>
        We can also save the model by adding that to the end of the program:
    </p>
    <p class="snippet">
model.save_model('saveXOR')
    </p>
    <p>
        And to load a model (to use it or to train it further) you just need to create an empty model like we did at the start and then do:
    </p>
    <p class="snippet">
model.load_model('saveXOR')
    </p>
    <p>
        There it is, we can just run the program and it will train!
    </p>
    </div>
</div>
</body>

</html> 